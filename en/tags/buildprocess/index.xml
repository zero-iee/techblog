<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>buildprocess on ZERO GmbH Tech Blog</title><link>https://blog.zero-iee.com/en/tags/buildprocess/</link><description>ZERO GmbH Tech Blog (buildprocess)</description><generator>Hugo -- gohugo.io</generator><language>de-de</language><lastBuildDate>Wed, 11 Jan 2023 14:19:06 +0100</lastBuildDate><atom:link href="https://blog.zero-iee.com/en/tags/buildprocess/index.xml" rel="self" type="application/rss+xml"/><item><title>Qt 5.15.2 with WebEngine (Chromium) - Limit RAM usage to avoid crashes</title><link>https://blog.zero-iee.com/en/posts/qt-5.15.2-mit-webengine-ram-begrenzen/</link><pubDate>Wed, 11 Jan 2023 14:19:06 +0100</pubDate><guid>https://blog.zero-iee.com/en/posts/qt-5.15.2-mit-webengine-ram-begrenzen/</guid><description>&lt;p>When compiling Qt 5.15.2 from the official open source sources, we encountered a problem in combination with our build server: The build process was interrupted while compiling the Chromium-based &amp;ldquo;WebEngine&amp;rdquo; component with initially mysterious error messages. A look at the kernel log using &lt;code>dmesg -w&lt;/code> then quickly revealed that the so-called OOM killer of the Linux kernel had struck. Apparently the RAM consumption of the build process was so memory-intensive that the process had to be aborted to keep the operating system running.&lt;/p>
&lt;p>But how could this be? Our build server has 32 GB of RAM and 24 CPU cores. A quite powerful machine. It should not actually reach its performance limits so quickly.&lt;/p>
&lt;p>The problem is caused by two factors. First, building the Chromium browser engine is extremely memory intensive. Generally, &lt;a href="https://chromium.googlesource.com/chromium/src/+/main/docs/linux/build_instructions.md">not less than 16 GB RAM is recommended&lt;/a>. In our case, however, there is another problem: By default, &amp;ldquo;Ninja&amp;rdquo; - the build system used in Chromium - creates a build thread for each available virtual CPU core, so that the build process is parallelized to the maximum. What may still work well for a standard PC with 16 GB RAM, however, forces our build server with its 24 cores to its knees. Every single thread needs a not to be underestimated amount of RAM. In the end, the ratio of CPU cores and available RAM is no longer correct on our server, so the build process stops.&lt;/p>
&lt;p>The problem can be prevented if we artificially reduce the number of Ninja threads to be used - if we don&amp;rsquo;t build with 24 CPU cores, for example, but only with 18.&lt;/p>
&lt;p>For this purpose the environment variable &lt;code>NINJAJOBS&lt;/code> can be set before a &lt;code>make -j$(nproc)&lt;/code>. Contrary to what one might expect &lt;em>(and contrary to what is described in the &lt;a href="https://www.linuxfromscratch.org/blfs/view/svn/x/qtwebengine.html">LFS manual&lt;/a>)&lt;/em>, however, not just a number is stored here, but the complete &lt;code>-j&lt;/code> &lt;a href="https://manpages.debian.org/testing/ninja-build/ninja.1.en.html">parameter of Ninja&lt;/a>:&lt;/p>
&lt;pre>&lt;code>export NINJAJOBS=&amp;quot;-j16&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>If a &lt;code>make&lt;/code> is subsequently executed, the usual Qt components are compiled with all cores, while the Ninja-based parts (in this case Chromium as part of the WebEngine) are built with fewer CPU cores to conserve RAM.&lt;/p>
&lt;p>For our combination of 32 GB RAM and 24 CPU cores, we experimentally determined a count of 16 kernels with which to still run our build process. With only 8 CPU cores, RAM usage peaked at about 12 GB.&lt;/p></description></item></channel></rss>